{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiffcmw/Maker-Portfolio/blob/main/Neural-Network-Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "So, this is going to be the code that was intended to be included in my research, but was pulled out for several reasons:\n",
        "\n",
        "1. I ran into a backend problem that made the quantized models unable to be deployed. Could be for other reasons, but I am puzzled till this day. (I would welcome any advice!)\n",
        "2. The paper has already reached beyond the page limit before I even got the chance to write about this section, and this section would likely require many words to eloquently explain.\n",
        "\n",
        "The research I did was about a detail examination and optimal quantizaiton method of the YOLOv8 object detection model.\n",
        "\n",
        "I will include anything that IS included in the research in the research supplement, but I thought it'll still be interesting to share the quantization process, since it's the actual implementation of the theories and is quite interesting how they are translated into code.\n",
        "\n",
        "The code here is just my process of doing quantizing the YOLOv8 model using the PyTorch framework (mainly). I did:\n",
        "\n",
        "* Static Post Training Quantization\n",
        "* Dynamic Post Training Quantization\n",
        "* Creating a dataloader\n",
        "\n",
        "None of this code (except for boilerplate codes used to import libs, packages, models) were submitted to the conference, so there is no conflicts with the blind review policy (in case you're one of the reviewers, who knows)."
      ],
      "metadata": {
        "id": "dX_EqI8r86A0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpOCPSGw5lbe"
      },
      "source": [
        "# Loading and Prepping the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul6kAF5tXWcp"
      },
      "outputs": [],
      "source": [
        "# install the ultralytics lib (they made yolov8)\n",
        "!pip install ultralytics\n",
        "\n",
        "# use wget to download the yolov8n model (it was trained using coco) from ths ultralytics git repository\n",
        "!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "16-i365-XH42"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "import torch\n",
        "import torch.quantization\n",
        "\n",
        "# .pt is a pytorch file, so might as well load it using torch\n",
        "# checkpoint should remain immutable in the session, because the model could be used from many purposes\n",
        "# checkpoint will be called to retrieve the original model\n",
        "checkpoint = torch.load(\"/content/yolov8n.pt\")\n",
        "\n",
        "# extract the state_dict\n",
        "model = checkpoint['model']\n",
        "\n",
        "# set the model to evaluation mode\n",
        "model = model.float().eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnggaK1aJNDv"
      },
      "outputs": [],
      "source": [
        "# function to create visual image (not needed yet)\n",
        "def imShow(path):\n",
        "  import cv2\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "  image = cv2.imread(path)\n",
        "  height, width = image.shape[:2]\n",
        "  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18, 10)\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otu8ef555Y8G"
      },
      "source": [
        "# Getting Calibration Data\n",
        "\n",
        "Creating Tensor List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WU6KPwU7TJb"
      },
      "source": [
        "## Downloading COCO Val2017"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9XR-7KEI1C0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa1b66f-9874-4f27-c53e-dcec52096047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-24 15:50:32--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.28.53, 3.5.25.201, 52.217.101.60, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.28.53|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘coco_val2017.zip’\n",
            "\n",
            "coco_val2017.zip    100%[===================>] 777.80M  16.2MB/s    in 62s     \n",
            "\n",
            "2023-12-24 15:51:34 (12.6 MB/s) - ‘coco_val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2023-12-24 15:51:34--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.163.113, 52.216.221.81, 52.216.49.177, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.163.113|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘coco_ann2017.zip’\n",
            "\n",
            "coco_ann2017.zip    100%[===================>] 241.19M  16.1MB/s    in 17s     \n",
            "\n",
            "2023-12-24 15:51:51 (14.6 MB/s) - ‘coco_ann2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# getting the coco val2017 dataset and annotations as the calibration dataset\n",
        "# this will take a while!\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4k6eqW8_1gqE"
      },
      "outputs": [],
      "source": [
        "# unzipping the downloaded files\n",
        "# I copied this off the internet\n",
        "\n",
        "from zipfile import ZipFile, BadZipFile\n",
        "import os\n",
        "\n",
        "def extract_zip_file(extract_path):\n",
        "    try:\n",
        "        with ZipFile(extract_path+\".zip\") as zfile:\n",
        "            zfile.extractall(extract_path)\n",
        "        # remove zipfile\n",
        "        zfileTOremove=f\"{extract_path}\"+\".zip\"\n",
        "        if os.path.isfile(zfileTOremove):\n",
        "            os.remove(zfileTOremove)\n",
        "        else:\n",
        "            print(\"Error: %s file not found\" % zfileTOremove)\n",
        "    except BadZipFile as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "extract_val_path = \"/content/coco_val2017\"\n",
        "extract_ann_path=\"/content/coco_ann2017\"\n",
        "\n",
        "extract_zip_file(extract_val_path)\n",
        "extract_zip_file(extract_ann_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_THYL-c3OPH",
        "outputId": "efc88567-d2e3-438e-dbc9-3ce19435a33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File count: 5000\n"
          ]
        }
      ],
      "source": [
        "# Check if I go the correct amount of calib data (should be 5000 for validation datset)\n",
        "import os\n",
        "\n",
        "# folder path\n",
        "dir_path = r\"/content/coco_val2017/val2017\"\n",
        "count = 0\n",
        "\n",
        "# Iterate directory\n",
        "for path in os.listdir(dir_path):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join(dir_path, path)):\n",
        "        count += 1\n",
        "\n",
        "# should be File count: 5000 if done right\n",
        "print('File count:', count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBObPsHfZjYp"
      },
      "source": [
        "## Creating Tensor List\n",
        "\n",
        "The list of tensor will be the photos. Tensorized photos get fed to the neural network - its just part of image processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Function to process images with specified range\n",
        "def process_images(folder_dir, start_index, end_index):\n",
        "    tensor_list = []\n",
        "\n",
        "    # defines a transformation function that transform the images to a uniform resolution of 640 * 640.\n",
        "    # it is the resolution that YOLOv8 accepts\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((640, 640)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # get sorted list of all images\n",
        "    images = sorted(os.listdir(folder_dir))\n",
        "\n",
        "    # slice the list for the desired range specified by start and end in the input parameter\n",
        "    selected_images = images[start_index:end_index]\n",
        "\n",
        "    for image_name in selected_images:\n",
        "        img_path = os.path.join(folder_dir, image_name)\n",
        "        img = Image.open(img_path).convert('RGB')  # Convert to RGB\n",
        "\n",
        "        # Apply the transformation\n",
        "        tensor = transform(img)\n",
        "\n",
        "        # Add another dimension at the front to get NCHW shape\n",
        "        tensor = tensor.unsqueeze(0)\n",
        "\n",
        "        # Add the tensorized photo to the list\n",
        "        tensor_list.append(tensor)\n",
        "\n",
        "    return tensor_list\n",
        "\n",
        "# Example usage:\n",
        "folder_dir = \"/content/coco_val2017/val2017\"\n",
        "# extract whatever range of images desired, I always do just 100 for the tensorlist or else runtime might crash\n",
        "# dataloaders will handle larger dataset usages\n",
        "\n",
        "start = 1   # Start from the first image (1st indexed, not 0 indexed)\n",
        "end = 101   # End at the 100th image\n",
        "\n",
        "tensor_list = process_images(folder_dir, start, end)"
      ],
      "metadata": {
        "id": "9EiX2_drFz2N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu_ZTUG1rxE_",
        "outputId": "1d636767-b75e-4715-c42e-c45dc54ab5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "# I found out that there are images that doensn't fit the criteria of [1, 3, 640, 640]\n",
        "# 3 layers (RGB) of size 640 x 640\n",
        "# The for loop interates through all the 100 items in the tensor list, and check\n",
        "\n",
        "# If all the images fit the criteria, 100 will be printed, nothing else\n",
        "# Images with defects will have their size and index number printed (for info)\n",
        "# Internally, the unfit list records the index of defected images and remove it from the tensor list\n",
        "\n",
        "# Again, it doesn't matter if the list is 100. The list is created just to have some images that can be easily accessed.\n",
        "# I also created it before the idea of a dataloader came with heavier workloads (and need for backward propagation customisation)\n",
        "\n",
        "unfit = []\n",
        "\n",
        "for i in range(0, len(tensor_list)):\n",
        "    tensor_list[i] = tensor_list[i].float()\n",
        "    # check if tensor is the right size\n",
        "    if tensor_list[i].size() != torch.Size([1, 3, 640, 640]):\n",
        "      print(tensor_list[i].size())\n",
        "      print(i)\n",
        "      unfit.append(i)\n",
        "\n",
        "# remove the tensors of unfit size\n",
        "for x in unfit:\n",
        "  tensor_list.pop(x)\n",
        "\n",
        "print(len(tensor_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY4O45FYfezT"
      },
      "source": [
        "## Creating Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the output is a list with three items. item 1 is a tensor of size torch.Size([64, 144, 80, 80]), second one is torch.Size([64, 144, 40, 40]), last one is torch.Size([64, 144, 20, 20])."
      ],
      "metadata": {
        "id": "xQ7NLbCA7kGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of an entry in instances_val2017\n",
        "\n",
        "import json\n",
        "\n",
        "# Open the JSON file for reading\n",
        "with open('/content/coco_ann2017/annotations/instances_val2017.json', 'r') as file:\n",
        "    # Parse the JSON data into a Python object\n",
        "    instances_val2017 = json.load(file)\n",
        "\n",
        "# Access the first image in the 'images' list\n",
        "first_image = instances_val2017['images'][0]\n",
        "\n",
        "# Print the first image dictionary\n",
        "print(first_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwiahnt4SRa3",
        "outputId": "509106c2-e343-4924-b5e2-fddf0a7a3129"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'license': 4, 'file_name': '000000397133.jpg', 'coco_url': 'http://images.cocodataset.org/val2017/000000397133.jpg', 'height': 427, 'width': 640, 'date_captured': '2013-11-14 17:02:52', 'flickr_url': 'http://farm7.staticflickr.com/6116/6255196340_da26cf2c9e_z.jpg', 'id': 397133}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_id = instances_val2017['images'][0]['id']\n",
        "\n",
        "# there is an annotation dictionary for each image in val2017, and each image has its onw image_id.\n",
        "# the for loop will iterate through all the annotations in the annotation section in the instances_val2017 dictionary\n",
        "# the annotations variable wil be initiated by the content of some annotation which image_id matching the id specified by the line above\n",
        "annotations = [item for item in instances_val2017['annotations'] if item['image_id'] == image_id]\n",
        "\n",
        "# Print the annotations for the first image\n",
        "for annotation in annotations:\n",
        "    print(annotation)\n",
        "\n",
        "# each segmentation dictionary is one object on the same image.\n",
        "# each object has its own bounding box coordinate (MAPPED TO THE ORIGINAL IMAGE)\n",
        "\n",
        "# SOOO basically the image size is not fit and the bounding boxes doesn't fit what it needed to be matched to the output of the neural network,\n",
        "# as part of loss calculation. sad.\n",
        "# Therefore, I had to create a function to reformat the bounding boxes. I'll not bother with the segmentation stuff because I don't need it\n",
        "# I just need to resive the bounding boxes using its original resolution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko5_rK3nV3iw",
        "outputId": "2712e087-937a-453c-95f1-86fc21b3a78e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'segmentation': [[224.24, 297.18, 228.29, 297.18, 234.91, 298.29, 243.0, 297.55, 249.25, 296.45, 252.19, 294.98, 256.61, 292.4, 254.4, 264.08, 251.83, 262.61, 241.53, 260.04, 235.27, 259.67, 230.49, 259.67, 233.44, 255.25, 237.48, 250.47, 237.85, 243.85, 237.11, 240.54, 234.17, 242.01, 228.65, 249.37, 224.24, 255.62, 220.93, 262.61, 218.36, 267.39, 217.62, 268.5, 218.72, 295.71, 225.34, 297.55]], 'area': 1481.3806499999994, 'iscrowd': 0, 'image_id': 397133, 'bbox': [217.62, 240.54, 38.99, 57.75], 'category_id': 44, 'id': 82445}\n",
            "{'segmentation': [[292.37, 425.1, 340.6, 373.86, 347.63, 256.31, 198.93, 240.24, 4.02, 311.57, 1.0, 427.0, 291.36, 427.0]], 'area': 54085.6217, 'iscrowd': 0, 'image_id': 397133, 'bbox': [1.0, 240.24, 346.63, 186.76], 'category_id': 67, 'id': 119568}\n",
            "{'segmentation': [[446.71, 70.66, 466.07, 72.89, 471.28, 78.85, 473.51, 88.52, 473.51, 98.2, 462.34, 111.6, 475.74, 126.48, 484.67, 136.16, 494.35, 157.74, 496.58, 174.12, 498.07, 182.31, 485.42, 189.75, 474.25, 189.01, 470.53, 202.4, 475.74, 337.12, 469.04, 347.54, 455.65, 343.08, 450.44, 323.72, 441.5, 255.99, 433.32, 250.04, 406.52, 340.1, 397.59, 344.56, 388.66, 330.42, 408.01, 182.31, 396.85, 186.77, 392.38, 177.84, 389.4, 166.68, 390.89, 147.32, 418.43, 119.04, 434.06, 111.6, 429.6, 98.94, 428.85, 81.08, 441.5, 72.89, 443.74, 69.92]], 'area': 17376.918849999995, 'iscrowd': 0, 'image_id': 397133, 'bbox': [388.66, 69.92, 109.41, 277.62], 'category_id': 1, 'id': 200887}\n",
            "{'segmentation': [[136.18, 253.44, 153.89, 277.3, 157.89, 278.22, 151.73, 269.6, 141.73, 252.67, 138.65, 249.74, 137.11, 249.43, 135.57, 249.43, 136.18, 253.13]], 'area': 123.19339999999974, 'iscrowd': 0, 'image_id': 397133, 'bbox': [135.57, 249.43, 22.32, 28.79], 'category_id': 49, 'id': 693231}\n",
            "{'segmentation': [[37.61, 381.77, 31.28, 360.25, 40.15, 352.65, 62.3, 345.69, 83.19, 346.32, 88.75, 344.0, 97.11, 345.78, 99.4, 348.57, 98.89, 359.47, 96.61, 371.14, 84.69, 380.26, 69.48, 381.79, 53.5, 384.83, 43.61, 383.81, 37.78, 381.53]], 'area': 2136.46615, 'iscrowd': 0, 'image_id': 397133, 'bbox': [31.28, 344.0, 68.12, 40.83], 'category_id': 51, 'id': 713388}\n",
            "{'segmentation': [[135.7, 296.93, 133.83, 304.16, 120.3, 320.72, 109.57, 325.86, 95.57, 328.66, 85.07, 327.26, 84.6, 316.29, 75.5, 310.69, 60.33, 304.86, 59.63, 303.69, 64.53, 298.09, 77.13, 291.56, 94.17, 288.76, 108.87, 287.36, 119.37, 288.99, 131.03, 292.03, 134.53, 294.36]], 'area': 2016.1509000000015, 'iscrowd': 0, 'image_id': 397133, 'bbox': [59.63, 287.36, 76.07, 41.3], 'category_id': 51, 'id': 716434}\n",
            "{'segmentation': [[1.78, 262.7, 193.92, 204.93, 166.71, 194.05, 142.43, 183.16, 136.57, 164.33, 2.2, 189.44, 1.36, 230.05]], 'area': 10058.87035, 'iscrowd': 0, 'image_id': 397133, 'bbox': [1.36, 164.33, 192.56, 98.37], 'category_id': 79, 'id': 1125079}\n",
            "{'segmentation': [[0.43, 299.58, 2.25, 299.58, 9.05, 287.78, 32.66, 299.13, 39.01, 296.4, 48.09, 290.96, 43.55, 286.87, 62.16, 291.86, 61.25, 286.87, 37.65, 279.15, 18.13, 272.8, 0.0, 262.81]], 'area': 1037.7818999999995, 'iscrowd': 0, 'image_id': 397133, 'bbox': [0.0, 262.81, 62.16, 36.77], 'category_id': 1, 'id': 1218137}\n",
            "{'segmentation': [[120.89, 289.63, 119.4, 287.15, 119.4, 283.18, 119.65, 281.44, 121.14, 280.45, 122.13, 279.21, 122.87, 276.73, 126.6, 274.0, 127.59, 273.25, 131.31, 272.51, 136.77, 274.0, 138.51, 279.71, 140.49, 295.59, 142.23, 299.56, 144.22, 301.54, 143.97, 302.78, 140.0, 305.02, 135.03, 306.26, 132.8, 306.76, 132.8, 304.77, 134.29, 301.05, 135.03, 298.57, 134.04, 294.6, 131.56, 292.11, 126.35, 289.88, 122.13, 289.38]], 'area': 415.37435000000005, 'iscrowd': 0, 'image_id': 397133, 'bbox': [119.4, 272.51, 24.82, 34.25], 'category_id': 47, 'id': 1878837}\n",
            "{'segmentation': [[147.29, 299.9, 146.13, 295.24, 143.41, 290.59, 141.47, 287.49, 141.47, 282.64, 142.06, 278.57, 143.41, 273.92, 147.87, 269.85, 154.27, 269.27, 160.47, 268.49, 165.13, 267.91, 167.84, 268.1, 170.17, 268.68, 171.33, 270.62, 171.72, 272.37, 173.66, 294.08, 173.27, 297.18, 170.17, 300.67, 155.82, 303.77]], 'area': 943.1977499999996, 'iscrowd': 0, 'image_id': 397133, 'bbox': [141.47, 267.91, 32.19, 35.86], 'category_id': 47, 'id': 1883614}\n",
            "{'segmentation': [[169.44, 186.08, 176.98, 185.17, 178.81, 183.57, 179.03, 182.88, 180.86, 177.86, 182.0, 173.52, 182.0, 170.78, 177.43, 171.69, 175.84, 171.92, 175.61, 171.24, 178.12, 168.95, 170.13, 168.95, 159.85, 169.87, 156.65, 171.24, 155.97, 173.98, 158.71, 180.37, 160.99, 181.74, 161.68, 183.34, 162.13, 184.71, 162.36, 186.08]], 'area': 351.9638500000004, 'iscrowd': 0, 'image_id': 397133, 'bbox': [155.97, 168.95, 26.03, 17.13], 'category_id': 51, 'id': 1902250}\n",
            "{'segmentation': [[160.14, 129.97, 171.49, 128.95, 173.79, 117.73, 174.68, 116.71, 175.06, 114.15, 157.2, 114.66]], 'area': 217.57500000000013, 'iscrowd': 0, 'image_id': 397133, 'bbox': [157.2, 114.15, 17.86, 15.82], 'category_id': 51, 'id': 1902971}\n",
            "{'segmentation': [[100.23, 310.35, 98.75, 309.28, 98.75, 306.98, 99.52, 305.8, 101.31, 305.34, 104.52, 304.83, 106.26, 304.78, 108.66, 304.83, 109.53, 306.47, 108.0, 307.18, 105.04, 307.33, 102.69, 307.9, 101.92, 308.87, 101.56, 309.84]], 'area': 31.567450000000104, 'iscrowd': 0, 'image_id': 397133, 'bbox': [98.75, 304.78, 10.78, 5.57], 'category_id': 56, 'id': 1914453}\n",
            "{'segmentation': [[170.68, 274.94, 174.85, 256.36, 168.82, 256.36, 166.03, 274.94]], 'area': 99.21719999999999, 'iscrowd': 0, 'image_id': 397133, 'bbox': [166.03, 256.36, 8.82, 18.58], 'category_id': 50, 'id': 2105658}\n",
            "{'segmentation': [[86.41, 299.65, 91.73, 305.15, 95.64, 305.15, 98.3, 304.44, 105.4, 302.66, 110.37, 300.36, 109.66, 297.16, 105.4, 299.47, 103.62, 300.89, 97.59, 300.0, 99.36, 295.03, 95.1, 293.97, 86.94, 297.52, 87.3, 300.53]], 'area': 136.74489999999983, 'iscrowd': 0, 'image_id': 397133, 'bbox': [86.41, 293.97, 23.96, 11.18], 'category_id': 56, 'id': 2114911}\n",
            "{'segmentation': [[79.42, 298.98, 74.72, 300.04, 72.26, 300.74, 70.85, 300.74, 70.14, 299.69, 73.55, 297.46, 77.42, 296.16, 79.18, 297.46]], 'area': 23.09860000000019, 'iscrowd': 0, 'image_id': 397133, 'bbox': [70.14, 296.16, 9.28, 4.58], 'category_id': 56, 'id': 2114949}\n",
            "{'segmentation': [[191.36, 210.9, 191.36, 244.56, 152.31, 261.4, 45.92, 304.49, 0.0, 309.88, 7.54, 291.02, 31.78, 297.76, 41.88, 286.98, 58.04, 293.04, 59.39, 287.66, 42.55, 280.92, 29.09, 272.84, 16.97, 272.17, 4.85, 266.11]], 'area': 7249.863299999998, 'iscrowd': 0, 'image_id': 397133, 'bbox': [0.0, 210.9, 191.36, 98.98], 'category_id': 79, 'id': 2139366}\n",
            "{'segmentation': [[98.2, 301.59, 100.47, 301.79, 102.38, 300.79, 104.05, 299.84, 104.53, 299.16, 103.97, 297.89, 102.78, 297.13, 101.54, 297.09, 99.0, 297.81, 97.48, 298.76, 96.69, 300.23, 97.05, 300.91, 97.88, 301.87, 99.31, 301.95]], 'area': 25.567200000000124, 'iscrowd': 0, 'image_id': 397133, 'bbox': [96.69, 297.09, 7.84, 4.86], 'category_id': 57, 'id': 2188144}\n",
            "{'segmentation': [[506.53, 203.4, 609.2, 210.55, 619.26, 232.01, 497.25, 222.73]], 'area': 2287.9704499999984, 'iscrowd': 0, 'image_id': 397133, 'bbox': [497.25, 203.4, 122.01, 28.61], 'category_id': 81, 'id': 2196309}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Dict\n",
        "\n",
        "def resize_bbox(bbox: List[float], original_size: Tuple[int, int], target_size: Tuple[int, int]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Resize the bounding box coordinates to match the target resolution.\n",
        "\n",
        "    Parameters:\n",
        "    # bbox is the original bbox [x_min, y_min, width, height] from 'segmentation'\n",
        "    # original size is (width, height) from 'image'\n",
        "    # target size is (whatever, whatever) target resolution e.g.(80, 80)\n",
        "\n",
        "    \"\"\"\n",
        "    # the x scale is the ratio between the original width and desired width\n",
        "    # the y scale is the ratio between the original height and desired height\n",
        "    scale_x = target_size[0] / original_size[0]\n",
        "    scale_y = target_size[1] / original_size[1]\n",
        "\n",
        "    # Apply the scale factors to the bbox coordinates [x_min, y_min, width, height]\n",
        "    resized_bbox = [bbox[0] * scale_x, bbox[1] * scale_y, bbox[2] * scale_x, bbox[3] * scale_y]\n",
        "\n",
        "    return resized_bbox"
      ],
      "metadata": {
        "id": "ob0eDOuodZRx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "\n",
        "# this is the first line in the code box before the previous\n",
        "# one of the bounding boxes of the first image in val2017\n",
        "original_bbox = annotations[0]['bbox']  # Original bounding box\n",
        "\n",
        "# 'width' and 'height' in first image in 'images' list from instances_val2017\n",
        "original_image_size = (first_image['width'], first_image['height'])  # Original image size (width, height)\n",
        "\n",
        "# Resize bounding box for 80x80 resolution\n",
        "bbox_80x80 = resize_bbox(original_bbox, original_image_size, (80, 80))\n",
        "\n",
        "# Resize bounding box for 40x40 resolution\n",
        "bbox_40x40 = resize_bbox(original_bbox, original_image_size, (40, 40))\n",
        "\n",
        "# Resize bounding box for 20x20 resolution\n",
        "bbox_20x20 = resize_bbox(original_bbox, original_image_size, (20, 20))\n",
        "\n",
        "print(\"Original BBox: \", original_bbox)\n",
        "print(\"80x80 BBox: \", bbox_80x80)\n",
        "print(\"40x40 BBox: \", bbox_40x40)\n",
        "print(\"20x20 BBox: \", bbox_20x20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtT-vBKmd3s9",
        "outputId": "ab5cd9a8-dcb5-4eb6-f0a0-a918e0003354"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original BBox:  [217.62, 240.54, 38.99, 57.75]\n",
            "80x80 BBox:  [27.2025, 45.066042154566745, 4.87375, 10.819672131147541]\n",
            "40x40 BBox:  [13.60125, 22.533021077283372, 2.436875, 5.409836065573771]\n",
            "20x20 BBox:  [6.800625, 11.266510538641686, 1.2184375, 2.7049180327868854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchvision"
      ],
      "metadata": {
        "id": "bmtWqUCUuIYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I discovered a COCO API for getting images instead, so doesn't need wget and download for runtime\n",
        "from pycocotools.coco import COCO\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "\n",
        "def resize_bbox(bbox: List[float], original_size: Tuple[int, int], target_size: Tuple[int, int]) -> List[float]:\n",
        "      \"\"\"\n",
        "      Resize the bounding box coordinates to match the target resolution.\n",
        "\n",
        "      Parameters:\n",
        "      # bbox is the original bbox [x_min, y_min, width, height] from 'segmentation'\n",
        "      # original size is (width, height) from 'image'\n",
        "      # target size is (whatever, whatever) target resolution e.g.(80, 80)\n",
        "\n",
        "      \"\"\"\n",
        "      # the x scale is the ratio between the original width and desired width\n",
        "      # the y scale is the ratio between the original height and desired height\n",
        "      scale_x = target_size[0] / original_size[0]\n",
        "      scale_y = target_size[1] / original_size[1]\n",
        "\n",
        "      # Apply the scale factors to the bbox coordinates [x_min, y_min, width, height]\n",
        "      resized_bbox = [bbox[0] * scale_x, bbox[1] * scale_y, bbox[2] * scale_x, bbox[3] * scale_y]\n",
        "\n",
        "      return resized_bbox\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, coco, image_ids, base_dir, transform=None):\n",
        "        # store COCO API interface for accessing the val2017 datset\n",
        "        self.coco = coco\n",
        "\n",
        "        # store list of image IDs used in this dataset instance\n",
        "        self.image_ids = image_ids\n",
        "\n",
        "        # store base directory of image location\n",
        "        self.base_dir = base_dir\n",
        "\n",
        "        # Store the transform(s) that will be applied to the images (e.g., for data augmentation)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Generate a list of image file paths using the COCO API and the provided image IDs.\n",
        "        # The COCO API's loadImgs method is called for each image ID (the image IDs are distinct for each image),\n",
        "        # and the 'file_name' is retrieved from the resulting dictionary, same as 'image'.\n",
        "        self.image_paths = [self.coco.loadImgs(id)[0]['file_name'] for id in self.image_ids]\n",
        "\n",
        "        # Generate a list of annotations for each image using the COCO API.\n",
        "        # The COCO API's getAnnIds method retrieves all annotation IDs for a given image ID,\n",
        "        # and then loadAnns retrieves the actual annotations for those IDs.\n",
        "        self.annotations = [self.coco.loadAnns(self.coco.getAnnIds(imgIds=id)) for id in self.image_ids]\n",
        "\n",
        "        # Run a method to filter out non-RGB images and their corresponding IDs and annotations.\n",
        "        # Ensures that only RGB images are used in the dataset.\n",
        "        self.image_paths, self.image_ids, self.annotations = self.filter_rgb_images_and_ids()\n",
        "\n",
        "    # A custom method that removes grayscale images or images with alpha channels\n",
        "    # I think I added this when I ran into lots of errors having wrong sizes of outputs or something\n",
        "    def filter_rgb_images_and_ids(self):\n",
        "      # Initialize empty lists to store paths, IDs, and annotations for RGB images only\n",
        "      rgb_image_paths = []\n",
        "      rgb_image_ids = []\n",
        "      rgb_annotations = []\n",
        "\n",
        "      # Iterate over all image paths, IDs, and annotations\n",
        "      for image_path, id, annotation in zip(self.image_paths, self.image_ids, self.annotations):\n",
        "          # Open the image file to check its mode\n",
        "          image = Image.open(os.path.join(self.base_dir, image_path))\n",
        "\n",
        "          # Check if the image is in RGB mode\n",
        "          if image.mode == 'RGB':\n",
        "              # If it is RGB, append the image path, ID, and annotation to their respective lists\n",
        "              rgb_image_paths.append(image_path)\n",
        "              rgb_image_ids.append(id)\n",
        "              rgb_annotations.append(annotation)\n",
        "\n",
        "          # Close the image file to free up resources\n",
        "          image.close()\n",
        "\n",
        "      # Return the lists containing only the information for RGB images\n",
        "      return rgb_image_paths, rgb_image_ids, rgb_annotations\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # Retrieve the file path for the image at the given index\n",
        "      image_path = self.image_paths[index]\n",
        "\n",
        "      # Open the image file using the path\n",
        "      image = Image.open(os.path.join(self.base_dir, image_path))\n",
        "\n",
        "      # Get the original size of the image as a (width, height) tuple\n",
        "      original_size = image.size\n",
        "\n",
        "      \"\"\"\n",
        "      Image Processing: Resize -> Transform to tensor\n",
        "      \"\"\"\n",
        "\n",
        "      # If a transform is set, apply it to the image\n",
        "      # the image needs to be resized to 640*640 for the input\n",
        "      if self.transform:\n",
        "          image = self.transform(image)\n",
        "\n",
        "      # If the image is not a PyTorch tensor, convert it to one\n",
        "      if not isinstance(image, torch.Tensor):\n",
        "          image = F.to_tensor(image)\n",
        "\n",
        "      \"\"\"\n",
        "      Annotation Processing: Resize + get category id\n",
        "      \"\"\"\n",
        "\n",
        "      # Retrieve the annotations for the current image\n",
        "      annotations = self.annotations[index]\n",
        "\n",
        "      # Initialize lists to hold the resized annotations for each scale\n",
        "      annotations_scales = {\n",
        "          '80': [],\n",
        "          '40': [],\n",
        "          '20': []\n",
        "      }\n",
        "\n",
        "      # The scales to which the annotations will be resized\n",
        "      target_scales = [80, 40, 20]\n",
        "\n",
        "      # Iterate over all annotations to resize them for each scale\n",
        "      for ann in annotations:\n",
        "          # Retrieve the original bounding box from the annotation\n",
        "          bbox = ann['bbox']\n",
        "          category_id = ann['category_id']\n",
        "\n",
        "          # Resize the bounding box for each target scale and store them in the annotations_scales dict\n",
        "          for scale in target_scales:\n",
        "              resized_bbox = resize_bbox(bbox, original_size, (scale, scale))\n",
        "              annotations_scales[str(scale)].append({'bbox': resized_bbox, 'category_id': category_id})\n",
        "\n",
        "      # Return the image tensor and the resized annotations for each scale\n",
        "      return image, {\n",
        "          'annotations_80': annotations_scales['80'],\n",
        "          'annotations_40': annotations_scales['40'],\n",
        "          'annotations_20': annotations_scales['20']\n",
        "      }\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of images in the dataset\n",
        "        return len(self.image_ids)\n",
        "\n",
        "def pad_tensors(tensor_list):\n",
        "    # Filter out any tensors with zero size in the second dimension\n",
        "    non_empty_tensors = [t for t in tensor_list if t.nelement() != 0]\n",
        "\n",
        "    if not non_empty_tensors:\n",
        "        # If all tensors are empty, return a placeholder tensor\n",
        "        # Adjust the dimensions and type as per your specific requirement\n",
        "        return torch.zeros((0, 0), dtype=torch.float32)\n",
        "    else:\n",
        "        # Pad the non-empty tensors\n",
        "        return pad_sequence(non_empty_tensors, batch_first=True)\n",
        "\n",
        "# size issues, related to batch size and paddings for uniformality in images\n",
        "def my_collate(batch):\n",
        "    # refer to the return statement above.\n",
        "    # the image is the first item\n",
        "    # the anotations is stored in a form of a dictionary, and the amount of annotations depends on the amount of objects there are\n",
        "    images = [item[0] for item in batch]\n",
        "    annotations_batch = [item[1] for item in batch]\n",
        "\n",
        "    # Prepare lists for bounding boxes for each scale and categories for all scales\n",
        "    bboxes_80 = []\n",
        "    bboxes_40 = []\n",
        "    bboxes_20 = []\n",
        "    categories = []  # Categories should be the same for all scales within each image\n",
        "\n",
        "    # Process each item in the batch\n",
        "    for annotations in annotations_batch:\n",
        "\n",
        "        # The categories are the same for all scales, because its just the same image with bounding boxes at different scales\n",
        "        # so take the set of categories from annotations_80\n",
        "        categories.append(torch.tensor([ann['category_id'] for ann in annotations['annotations_80']], dtype=torch.int64))\n",
        "\n",
        "        # Process bounding boxes for each scale\n",
        "        bboxes_80.append(torch.tensor([ann['bbox'] for ann in annotations['annotations_80']], dtype=torch.float32))\n",
        "        bboxes_40.append(torch.tensor([ann['bbox'] for ann in annotations['annotations_40']], dtype=torch.float32))\n",
        "        bboxes_20.append(torch.tensor([ann['bbox'] for ann in annotations['annotations_20']], dtype=torch.float32))\n",
        "\n",
        "    # Use the utility function to pad each set\n",
        "    bboxes_80_padded = pad_tensors(bboxes_80)\n",
        "    bboxes_40_padded = pad_tensors(bboxes_40)\n",
        "    bboxes_20_padded = pad_tensors(bboxes_20)\n",
        "    categories_padded = pad_tensors(categories)\n",
        "\n",
        "    # Stack images into a single tensor\n",
        "    images_stacked = torch.stack(images)\n",
        "\n",
        "    # Return the collated batch\n",
        "    return images_stacked, {\n",
        "        'bboxes_80': bboxes_80_padded,\n",
        "        'bboxes_40': bboxes_40_padded,\n",
        "        'bboxes_20': bboxes_20_padded,\n",
        "        'categories': categories_padded  # Categories are the same for all scales\n",
        "    }\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(640),  # Resize the smaller edge to 640\n",
        "    transforms.CenterCrop((640, 640)),  # Crop the center to make the image 640x640\n",
        "    transforms.ToTensor()  # Convert to tensor\n",
        "])"
      ],
      "metadata": {
        "id": "6SUH8hLwC-1i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the images\n",
        "image_directory = '/content/coco_val2017/val2017'\n",
        "\n",
        "# Full path to the annotations\n",
        "annotation_file = '/content/coco_ann2017/annotations/instances_val2017.json'\n",
        "\n",
        "# Creates an instance of the COCO class from pycocotools.coco import COCO\n",
        "# it parses instances_val2017 without needing to manually access it through dictionaries and lists\n",
        "coco = COCO(annotation_file)\n",
        "\n",
        "# Get ids of all images in the dataset instantiated by COCO(annotation file)\n",
        "image_ids = coco.getImgIds()\n",
        "\n",
        "# Assume image_ids is a list of all image ids\n",
        "# random.shuffle(image_ids) # for shuffled order\n",
        "\n",
        "# Use only the first x images\n",
        "image_ids_subsampled = image_ids[:100]\n",
        "\n",
        "# passes the retrieved info into the COCODataset class defined above\n",
        "dataset = COCODataset(coco, image_ids_subsampled, image_directory, transform=transform)\n",
        "\n",
        "# uses the dataloader module from torch.utils.data to create a dataloader\n",
        "\"\"\"\n",
        "batch_size=64: 64 samples for each iteration\n",
        "num_workers=4: Four parallel workers will load the data to increase speed\n",
        "pin_memory=True: Performance optimization for data transfer to CUDA-enabled GPUs.(me with cloud gpu subscriptions)\n",
        "prefetch_factor=2: 2 batches are preloaded in advance. Can improve performance by overlapping data loading with computation.\n",
        "shuffle=True: The dataset will be shuffled at the start of each epoch, which is good practice for training\n",
        "collate_fn=my_collate: Uses a custom collate function from the COCODataset class, to handle variable numbers of objects and annotations per image.\n",
        "\"\"\"\n",
        "dataloader = DataLoader(dataset, batch_size=64, num_workers=4, pin_memory=True, prefetch_factor=2, shuffle=True, collate_fn=my_collate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoVTwYB-pZKG",
        "outputId": "d35c5bbd-42d7-435b-e3bd-3519dd71742c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.72s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lph4SGWJdYzm"
      },
      "source": [
        "# Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-yzhKmweepr"
      },
      "source": [
        "## Static Post Training Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NVrLsN9DjLta"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.ao.quantization.observer import MinMaxObserver\n",
        "\n",
        "# backstory: in my research, I proposed the optimal clipping range for weights to utilise 3 standard deviations away from the mean,\n",
        "# upon fitting the activation values onto a Laplace Distribution.\n",
        "# On the other hand, due to the skewed distribution of activations, using percentiles (0.5th and 99.5th) for clipping range instead of fitting them to curves,\n",
        "# seemed like the better move and offers better accuracy.\n",
        "\n",
        "# the observer class is able to extract the observed values at a given moment based on the prescribed instructions\n",
        "\n",
        "# Define a class PercentileObserver that inherits from MinMaxObserver\n",
        "class PercentileObserver(MinMaxObserver):\n",
        "    # Constructor with default percentiles set to 0.5 and 99.5\n",
        "    def __init__(self, min_percentile=0.5, max_percentile=99.5, **kwargs):\n",
        "        super().__init__(**kwargs)  # Initialize the base class with any additional keyword arguments\n",
        "        self.min_percentile = min_percentile  # Set the minimum percentile threshold\n",
        "        self.max_percentile = max_percentile  # Set the maximum percentile threshold\n",
        "\n",
        "    # Method called when a batch of data x is passed through the observer\n",
        "    def forward(self, x):\n",
        "        # Calculate the value at the minimum percentile\n",
        "        min_val, _ = x.view(-1).kthvalue(int(x.numel() * self.min_percentile / 100))\n",
        "        # Calculate the value at the maximum percentile\n",
        "        max_val, _ = x.view(-1).kthvalue(int(x.numel() * self.max_percentile / 100))\n",
        "        # Update self.min_val to be the minimum between the existing min_val and the new min_val\n",
        "        self.min_val = min(self.min_val, min_val)\n",
        "        # Update self.max_val to be the maximum between the existing max_val and the new max_val\n",
        "        self.max_val = max(self.max_val, max_val)\n",
        "        # Return the unchanged input data\n",
        "        return x\n",
        "\n",
        "# Define a class LaplaceObserver that also inherits from MinMaxObserver\n",
        "class LaplaceObserver(MinMaxObserver):\n",
        "    # Constructor with a default number of standard deviations set to 3\n",
        "    def __init__(self, num_stddev=3, **kwargs):\n",
        "        super().__init__(**kwargs)  # Initialize the base class with any additional keyword arguments\n",
        "        self.num_stddev = num_stddev  # Set the number of standard deviations for range estimation\n",
        "\n",
        "    # Method called when a batch of data x is passed through the observer\n",
        "    def forward(self, x):\n",
        "        mean = x.mean()  # Calculate the mean of the input data\n",
        "        std = x.std()  # Calculate the standard deviation of the input data\n",
        "        # Set the minimum observed value, ensuring it does not go below mean - num_stddev * std\n",
        "        self.min_val = max(self.min_val, mean - self.num_stddev * std)\n",
        "        # Set the maximum observed value, ensuring it does not exceed mean + num_stddev * std\n",
        "        self.max_val = min(self.max_val, mean + self.num_stddev * std)\n",
        "        # Return the unchanged input data\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PpOt_4iJeiRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c39381-34d4-4773-c191-68f24a480e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DetectionModel(\n",
            "  (model): Sequential(\n",
            "    (0): Conv(\n",
            "      (conv): QuantizedConv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), scale=0.0044291215017437935, zero_point=5, padding=(1, 1), bias=False)\n",
            "      (bn): QuantizedBatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (1): Conv(\n",
            "      (conv): QuantizedConv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.0473511628806591, zero_point=5, padding=(1, 1), bias=False)\n",
            "      (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (2): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.07088109105825424, zero_point=25, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.028191180899739265, zero_point=26, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.017135735601186752, zero_point=25, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.033653125166893005, zero_point=1, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): Conv(\n",
            "      (conv): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.022563941776752472, zero_point=22, padding=(1, 1), bias=False)\n",
            "      (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (4): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.009890656918287277, zero_point=1, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.012215829454362392, zero_point=19, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.00934519898146391, zero_point=51, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.006209663115441799, zero_point=-8, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.01029269676655531, zero_point=39, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.0034421836026012897, zero_point=-31, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (5): Conv(\n",
            "      (conv): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.008935105986893177, zero_point=18, padding=(1, 1), bias=False)\n",
            "      (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (6): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.005019059870392084, zero_point=4, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.009522630833089352, zero_point=19, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.006450188346207142, zero_point=39, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.005477422848343849, zero_point=-5, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.007476631086319685, zero_point=28, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.004368798807263374, zero_point=-16, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (7): Conv(\n",
            "      (conv): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.005923184100538492, zero_point=8, padding=(1, 1), bias=False)\n",
            "      (bn): QuantizedBatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (8): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.003387044882401824, zero_point=-9, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.007543577812612057, zero_point=30, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004537228960543871, zero_point=1, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.011381994932889938, zero_point=48, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (9): SPPF(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.005007096100598574, zero_point=1, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.018721455708146095, zero_point=4, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (10): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (11): Concat()\n",
            "    (12): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.004628551658242941, zero_point=-8, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.004676307085901499, zero_point=5, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.00709201954305172, zero_point=19, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.003657634137198329, zero_point=-16, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (13): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (14): Concat()\n",
            "    (15): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.005438480991870165, zero_point=3, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.005318208131939173, zero_point=24, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.007135794963687658, zero_point=23, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.003919614013284445, zero_point=-1, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (16): Conv(\n",
            "      (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), scale=0.006200734525918961, zero_point=9, padding=(1, 1), bias=False)\n",
            "      (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (17): Concat()\n",
            "    (18): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.003938253968954086, zero_point=-2, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.0044695730321109295, zero_point=7, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.006261992733925581, zero_point=31, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0033285398967564106, zero_point=2, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (19): Conv(\n",
            "      (conv): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.003990549594163895, zero_point=6, padding=(1, 1), bias=False)\n",
            "      (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (20): Concat()\n",
            "    (21): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): QuantizedConv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0024765287525951862, zero_point=-1, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): QuantizedConv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.003392874263226986, zero_point=18, bias=False)\n",
            "        (bn): QuantizedBatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004451519809663296, zero_point=12, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0038131624460220337, zero_point=-5, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (22): Detect(\n",
            "      (cv2): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.005015384405851364, zero_point=14, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0037401069421321154, zero_point=-10, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.044164448976516724, zero_point=-69)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): QuantizedConv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0043619233183562756, zero_point=13, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.006375324446707964, zero_point=-11, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.04378969967365265, zero_point=-67)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): QuantizedConv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.005038153380155563, zero_point=-10, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008922362700104713, zero_point=-25, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.048762861639261246, zero_point=-60)\n",
            "        )\n",
            "      )\n",
            "      (cv3): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): QuantizedConv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), scale=0.006528389640152454, zero_point=38, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): QuantizedConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), scale=0.004891866352409124, zero_point=32, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): QuantizedConv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.07700864970684052, zero_point=127)\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): QuantizedConv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), scale=0.0049062687903642654, zero_point=7, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): QuantizedConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), scale=0.006258881185203791, zero_point=46, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): QuantizedConv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.08816270530223846, zero_point=127)\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): QuantizedConv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), scale=0.0044720787554979324, zero_point=-12, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): QuantizedConv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), scale=0.007304030936211348, zero_point=40, padding=(1, 1), bias=False)\n",
            "            (bn): QuantizedBatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): QuantizedConv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.09002965688705444, zero_point=127)\n",
            "        )\n",
            "      )\n",
            "      (dfl): DFL(\n",
            "        (conv): QuantizedConv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.04361680522561073, zero_point=-128, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:317: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.quantization\n",
        "\n",
        "# Load pre-trained model\n",
        "checkpoint = torch.load(\"/content/yolov8n.pt\")\n",
        "\n",
        "# Extract the state_dict\n",
        "model = checkpoint['model'].float().eval()\n",
        "\n",
        "model.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
        "# Update the qconfig for activation and weight to use your custom observers\n",
        "model.qconfig = torch.ao.quantization.qconfig.QConfig(\n",
        "    activation=PercentileObserver.with_args(min_percentile=0.5, max_percentile=99.5, dtype=torch.qint8),\n",
        "    weight=LaplaceObserver.with_args(num_stddev=3, dtype=torch.qint8)\n",
        ")\n",
        "\n",
        "# Prepare the model for static quantization\n",
        "model_static_quant = torch.quantization.prepare(model)\n",
        "\n",
        "# Define a calibration function\n",
        "def calibrate(model, loader):\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for images, _ in loader:\n",
        "            model(images)\n",
        "\n",
        "# i could've used less photos but 1.5 min is okay :)\n",
        "# Calibrate the model using your data loader\n",
        "calibrate(model_static_quant, dataloader)\n",
        "\n",
        "# Convert the model to a quantized version\n",
        "model_static_quant = torch.quantization.convert(model_static_quant)\n",
        "\n",
        "print(model_static_quant)  # Print the quantized model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = tensor_list[0]\n",
        "model_static_quant(dummy_input)\n",
        "\n",
        "# so yeah this is the error I was talking about :(\n",
        "# the SAME ONE i was seeing for a solid week\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rKVKkDTfE5uE",
        "outputId": "24a2162c-9d0b-41a5-fe46-806ac2550eed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-415e4aad8a30>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_static_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# so yeah this is the error I was talking about :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the SAME ONE i was seeing for a solid week\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for cases of training and validating while training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36m_predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;34m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    466\u001b[0m             input = F.pad(input, _reversed_padding_repeated_twice,\n\u001b[1;32m    467\u001b[0m                           mode=self.padding_mode)\n\u001b[0;32m--> 468\u001b[0;31m         return ops.quantized.conv2d(\n\u001b[0m\u001b[1;32m    469\u001b[0m             input, self._packed_params, self.scale, self.zero_point)\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nQuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1874 [kernel]\nQuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:388 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:296 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:710 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qS-E7ckXh3Q"
      },
      "source": [
        "## Dynamic Post Training Quantization\n",
        "\n",
        "PyTorch only supports these modules:\n",
        "- nn.Linear\n",
        "- nn.LSTM\n",
        "- nn.GRU\n",
        "- nn.LSTMCell\n",
        "- nn.RNNCell\n",
        "- nn.GRUCell\n",
        "\n",
        "None of which are used in YOLOv8.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TpintxibNeU"
      },
      "source": [
        "### PyTorch\n",
        "\n",
        "Hence and thereof, the default dynamic quantization does not work.\n",
        "\n",
        "In the documentations, the srcs often fuse the modules before quantization to increase efficiency even more but SiLU is not supported, and just fusing Conv2d and BatchNorm2d and leaving SiLU out there seemed weird so I just didn't bother fusing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GebOy6SOXxlc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.quantization\n",
        "\n",
        "model = checkpoint['model'].float().eval()\n",
        "\n",
        "# create a quantized model instance\n",
        "model_int8 = torch.ao.quantization.quantize_dynamic(\n",
        "    model,  # the original model  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lSfeBul_ZNE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276e7384-a921-4ba2-b568-20682c3f71d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DetectionModel(\n",
            "  (model): Sequential(\n",
            "    (0): Conv(\n",
            "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (1): Conv(\n",
            "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (2): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): Conv(\n",
            "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (4): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0-1): 2 x Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (5): Conv(\n",
            "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (6): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0-1): 2 x Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (7): Conv(\n",
            "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (8): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (9): SPPF(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (10): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (11): Concat()\n",
            "    (12): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (13): Upsample(scale_factor=2.0, mode='nearest')\n",
            "    (14): Concat()\n",
            "    (15): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (16): Conv(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (17): Concat()\n",
            "    (18): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (19): Conv(\n",
            "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "      (act): SiLU(inplace=True)\n",
            "    )\n",
            "    (20): Concat()\n",
            "    (21): C2f(\n",
            "      (cv1): Conv(\n",
            "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (cv2): Conv(\n",
            "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (m): ModuleList(\n",
            "        (0): Bottleneck(\n",
            "          (cv1): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (cv2): Conv(\n",
            "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (22): Detect(\n",
            "      (cv2): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (cv3): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Conv(\n",
            "            (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (1): Conv(\n",
            "            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "            (act): SiLU(inplace=True)\n",
            "          )\n",
            "          (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (dfl): DFL(\n",
            "        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model_int8)\n",
        "\n",
        "# none of the modules are quantized\n",
        "# same as original"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VKwNhnkbR_c"
      },
      "source": [
        "### ONNX Export\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "9qu8dAbwkf66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Check if CUDA is available\n",
        "# FP16 export is only possible if CUDA-capable GPU is being used\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')  # Set the device to GPU (device index 0)\n",
        "\n",
        "    # Load your model\n",
        "    model = torch.load('yolov8n.pt')['model'].float().eval()  # Adjust as necessary for your model loading\n",
        "    model.to(device)  # Move your model to the GPU\n",
        "\n",
        "    # convert the model to half precision, which is float16 (by default its float32)\n",
        "    model.half()\n",
        "\n",
        "    # Createthen move the tensor to GPU and change it to half precision\n",
        "    # (so it can be passed to the FP16 model in GPU)\n",
        "    input_tensor = torch.randn(1, 3, 640, 640, device=device).half()\n",
        "\n",
        "    # Export the model to ONNX\n",
        "    torch.onnx.export(model,\n",
        "                      input_tensor,\n",
        "                      'yolov8n.onnx',\n",
        "                      export_params=True,\n",
        "                      opset_version=17,  # The ONNX opset version to export the model with\n",
        "                      do_constant_folding=True,\n",
        "                      input_names=['input'],\n",
        "                      output_names=['output'],\n",
        "                      dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n",
        "else:\n",
        "    print(\"CUDA is not available. GPU export cannot be performed.\")"
      ],
      "metadata": {
        "id": "xAfGE7iUJY7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7af0846-8b82-4cad-c6b2-a85232bcf821"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# Load the ONNX model\n",
        "model = onnx.load(\"/content/yolov8n.onnx\")\n",
        "\n",
        "# Print a human-readable representation of the model\n",
        "print(onnx.helper.printable_graph(model.graph))\n",
        "\n",
        "# Quantized, FLOAT16 is used across the board"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSMrq2wCLvep",
        "outputId": "80ad2406-4ac0-4c29-c7f3-7ff3a9171b6d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph main_graph (\n",
            "  %input[FLOAT16, batch_sizex3x640x640]\n",
            ") initializers (\n",
            "  %model.22.cv2.0.2.weight[FLOAT16, 64x64x1x1]\n",
            "  %model.22.cv2.0.2.bias[FLOAT16, 64]\n",
            "  %model.22.cv2.1.2.weight[FLOAT16, 64x64x1x1]\n",
            "  %model.22.cv2.1.2.bias[FLOAT16, 64]\n",
            "  %model.22.cv2.2.2.weight[FLOAT16, 64x64x1x1]\n",
            "  %model.22.cv2.2.2.bias[FLOAT16, 64]\n",
            "  %model.22.cv3.0.2.weight[FLOAT16, 80x80x1x1]\n",
            "  %model.22.cv3.0.2.bias[FLOAT16, 80]\n",
            "  %model.22.cv3.1.2.weight[FLOAT16, 80x80x1x1]\n",
            "  %model.22.cv3.1.2.bias[FLOAT16, 80]\n",
            "  %model.22.cv3.2.2.weight[FLOAT16, 80x80x1x1]\n",
            "  %model.22.cv3.2.2.bias[FLOAT16, 80]\n",
            "  %model.22.dfl.conv.weight[FLOAT16, 1x16x1x1]\n",
            "  %onnx::Conv_986[FLOAT16, 16x3x3x3]\n",
            "  %onnx::Conv_987[FLOAT16, 16]\n",
            "  %onnx::Conv_989[FLOAT16, 32x16x3x3]\n",
            "  %onnx::Conv_990[FLOAT16, 32]\n",
            "  %onnx::Conv_992[FLOAT16, 32x32x1x1]\n",
            "  %onnx::Conv_993[FLOAT16, 32]\n",
            "  %onnx::Conv_995[FLOAT16, 16x16x3x3]\n",
            "  ......\n",
            "  %onnx::Conv_1154[FLOAT16, 80x80x3x3]\n",
            "  %onnx::Conv_1155[FLOAT16, 80]\n",
            ") {\n",
            "  %/model.0/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%input, %onnx::Conv_986, %onnx::Conv_987)\n",
            "  %/model.0/act/Sigmoid_output_0 = Sigmoid(%/model.0/conv/Conv_output_0)\n",
            "  %/model.0/act/Mul_output_0 = Mul(%/model.0/conv/Conv_output_0, %/model.0/act/Sigmoid_output_0)\n",
            "  %/model.1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%/model.0/act/Mul_output_0, %onnx::Conv_989, %onnx::Conv_990)\n",
            "  %/model.1/act/Sigmoid_output_0 = Sigmoid(%/model.1/conv/Conv_output_0)\n",
            "  %/model.1/act/Mul_output_0 = Mul(%/model.1/conv/Conv_output_0, %/model.1/act/Sigmoid_output_0)\n",
            "  %/model.2/cv1/conv/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%/model.1/act/Mul_output_0, %onnx::Conv_992, %onnx::Conv_993)\n",
            "  %/model.2/cv1/act/Sigmoid_output_0 = Sigmoid(%/model.2/cv1/conv/Conv_output_0)\n",
            "  %/model.2/cv1/act/Mul_output_0 = Mul(%/model.2/cv1/conv/Conv_output_0, %/model.2/cv1/act/Sigmoid_output_0)\n",
            "  %/model.2/Shape_output_0 = Shape(%/model.2/cv1/act/Mul_output_0)\n",
            "  %/model.2/Constant_output_0 = Constant[value = <Tensor>]()\n",
            "  %/model.2/Gather_output_0 = Gather[axis = 0](%/model.2/Shape_output_0, %/model.2/Constant_output_0)\n",
            "  %/model.2/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
            "  %/model.2/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
            "  %/model.2/Add_output_0 = Add(%/model.2/Gather_output_0, %/model.2/Constant_2_output_0)\n",
            "  %/model.2/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
            "  %/model.2/Div_output_0 = Div(%/model.2/Add_output_0, %/model.2/Constant_3_output_0)\n",
            "  %/model.2/Constant_4_output_0 = Constant[value = <Tensor>]()\n",
            "  %/model.2/Mul_output_0 = Mul(%/model.2/Div_output_0, %/model.2/Constant_4_output_0)\n",
            "  %/model.2/Slice_output_0 = Slice(%/model.2/cv1/act/Mul_output_0, %/model.2/Constant_1_output_0, %/model.2/Mul_output_0, %/model.2/Constant_output_0)\n",
            "  %/model.2/Constant_5_output_0 = Constant[value = <Tensor>]()\n",
            "  %/model.2/Mul_1_output_0 = Mul(%/model.2/Div_output_0, %/model.2/Constant_5_output_0)\n",
            "  %/model.2/Slice_1_output_0 = Slice(%/model.2/cv1/act/Mul_output_0, %/model.2/Mul_output_0, %/model.2/Mul_1_output_0, %/model.2/Constant_output_0)\n",
            "  ...... \n",
            "  %/model.22/Sigmoid_output_0 = Sigmoid(%/model.22/Split_1_output_1)\n",
            "  %output = Concat[axis = 1](%/model.22/Mul_5_output_0, %/model.22/Sigmoid_output_0)\n",
            "  return %output, %onnx::Shape_702, %onnx::Shape_721, %onnx::Shape_740\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also did it with Tensorflow, but it requires lower version of both Python. I don't want to mess with different dependencies and environments in a singular colab file so I will not include it here."
      ],
      "metadata": {
        "id": "y4dUnRfRPRy1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tBObPsHfZjYp",
        "wuPjsgd2LOUm",
        "YLv-Vf-T7W2E",
        "2TpintxibNeU",
        "MZAuTekN5dYT"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "17xrm4Kr-rJ23sLr2zir9vv7ho3hbUrbe",
      "authorship_tag": "ABX9TyM1vNFZmrcC2ACF4FsAPiXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
